{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Key LLM Training Steps:**\n",
        "1. `Stage 1:` Pre-training: Here, model learns to generate text via next token prediciton (`Token | Probabibility`). [Flow: Pre-trained dataset or random weights or untrained model -> LLM for pretraining -> Pre-trained LLM Base model]\n",
        "\n",
        "2. `Stage 2:` Mid or continuous pre-training: More specific to curated dataset, mostly well established datasets to train the model. (Can increase context length limits)\n",
        "\n",
        "3. `Stage 3:` Post-training: Here we use fine-tuning model for given input to a target output. It uses SFT for language models; another method is RL that teaches the model on certain input, whether its response was good or bad based on some reward/score.\n",
        "\n",
        "\n",
        "**Fine-Tuning & RL:**\n",
        "\n",
        "- Fine-tuning: You need good data `{input, output}`\n",
        "- RL: You need input data, not need target output\n",
        "\n",
        "**Flow:** Base LLM Model -> Fine-tuning -> RL -> Fine-tuned Model\n",
        "\n",
        "**Steps**:\n",
        "1. Get fine-tuned data `{input, target output}`\n",
        "2. Fine tune LLM -> Fine-tuned LLM\n",
        "3. Create RL training environment with `{input}` data, or other info like files, pdf, tools, etc\n",
        "4. RL loop then get RL data `{input, output, reward}` in RL training environments and then train the fine-tuned LLM with RL to get New model\n",
        "\n",
        "**For reasoning LLM model:**\n",
        "- We fine-tune with chain of thoughts CoT, so we use fine-tune on `{input, think+answer}` CoT dataset for a specific domain to train.\n",
        "- Secondly, RL for reasoning, we add RL Training environments with verifiers adn reward models to get new models.\n",
        "\n",
        "**Flow:** Base LLM model -> Fine tuning CoT for reasoning -> RL reasoning -> Fine tuning Mix reasoning + non-reasoning -> RL mix reasoning + non-reasoning -> New LLM Model\n",
        "\n",
        "**Model Safety and security during Post-training LLM using RL with AI feedback or RLAIF**"
      ],
      "metadata": {
        "id": "oadl3VmPeLJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "M87rJ4A5jDXC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h7hVfFod4cG"
      },
      "outputs": [],
      "source": []
    }
  ]
}